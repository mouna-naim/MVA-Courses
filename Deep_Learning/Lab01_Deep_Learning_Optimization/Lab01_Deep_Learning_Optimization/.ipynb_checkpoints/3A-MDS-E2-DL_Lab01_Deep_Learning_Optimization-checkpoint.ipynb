{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3A-MDS-E2-DL Deep Learning Optimization: Lab01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read carefully all the defined functions.\n",
    "\n",
    "Answer to questions filling the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from random import seed\n",
    "from random import random\n",
    "from math import exp\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from utils import *\n",
    "\n",
    "# Initialize the pseudo-random number generator \n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. A simple classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we define a very simple $2D$ dataset with $10$ samples belonging to $2$ classes.\n",
    "\n",
    "In this section we will have to generate a simple dense neural network and fill the functions of backpropagation for its optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdataset = [[2.7810836,2.550537003,0],\n",
    "            [1.465489372,2.362125076,0],\n",
    "            [3.396561688,4.400293529,0],\n",
    "            [1.38807019,1.850220317,0],\n",
    "            [7.627531214,2.759262235,1],\n",
    "            [5.332441248,2.088626775,1],\n",
    "            [6.922596716,1.77106367,1],\n",
    "            [8.675418651,-0.242068655,1],\n",
    "            [7.673756466,3.508563011,1],\n",
    "            [3.06407232,3.005305973,0]]\n",
    "\n",
    "dataset_np = np.array(sdataset)\n",
    "fig, ax = plt.subplots()\n",
    "colors = ['blue', 'orange']\n",
    "scatter = ax.scatter(dataset_np[:,0], dataset_np[:,1], c=dataset_np[:,2], s=1e2, edgecolors='none')\n",
    "legend = ax.legend(*scatter.legend_elements(),\n",
    "                    loc=\"lower left\", title=\"Classes\")\n",
    "ax.add_artist(legend)\n",
    "ax.grid(True)\n",
    "ax.set_title('A simple dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##     I.1. Optimization of a simple dense neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###         I.1.a. Initialization of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's define our deep neural network.\n",
    "\n",
    "As parameters, we will use the size of the input, the number of hidden layers, their respective size and the size of the output.\n",
    "\n",
    "Our network will be defined as a list (layers) of list (neurons) of dictionnaries (weights and other variables to store)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a neural network with dense layers\n",
    "def initialize_dense_network(size_inputs, n_hidden_layers, sizes_hidden_layers, size_outputs):\n",
    "    network = []\n",
    "    \n",
    "    # Input layer\n",
    "    input_layer = []\n",
    "    for j in range(sizes_hidden_layers[0]):\n",
    "        neuron =  {'weights': [random() for i in range(size_inputs+1)]}\n",
    "        input_layer.append(neuron)\n",
    "    network.append(input_layer)\n",
    "    size_inputs = len(network[-1])\n",
    "    \n",
    "    # Hidden layers\n",
    "    for layer_output_size in sizes_hidden_layers[1:]:\n",
    "        hidden_layer = []\n",
    "        for j in range(layer_output_size):\n",
    "            neuron = {'weights': [random() for i in range(size_inputs+1)]}\n",
    "            hidden_layer.append(neuron)\n",
    "        network.append(hidden_layer)\n",
    "        size_inputs = len(network[-1])\n",
    "        \n",
    "    # Output layer\n",
    "    output_layer = []\n",
    "    for j in range(size_outputs):\n",
    "        neuron = {'weights': [random() for i in range(size_inputs + 1)]}\n",
    "        output_layer.append(neuron)\n",
    "    network.append(output_layer)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of the dense neural network\n",
    "size_inputs = 3\n",
    "n_hidden_layers = 1\n",
    "sizes_hidden_layers =  [2]\n",
    "size_outputs = 2\n",
    "\n",
    "# Initialize network\n",
    "network = initialize_dense_network(size_inputs, n_hidden_layers, sizes_hidden_layers, size_outputs)\n",
    "\n",
    "# Plot network\n",
    "plot_dense_network(size_inputs, n_hidden_layers, sizes_hidden_layers, size_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1: Why the first dimension has +1 length in the initialization of our dense network?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1.b Forward pass of an input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, you can have a look on the way that we discussed backpropagation in the class, for a $3$-layer network using an least-squares loss.\n",
    "\n",
    "Lets start with the forward pass. For this experiment we will use sigmoid as the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"slide_back_propagation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1.b Forward pass of an input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, you can have a look on the way that we discussed backpropagation in the class, for a $3$-layer network using an least-squares loss.\n",
    "\n",
    "Lets start with the forward pass. For this experiment we will use sigmoid as the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### # Calculate neuron activation for an input from its weights\n",
    "def activate_neuron(weights, inputs):\n",
    "    bias = weights[-1]\n",
    "    activation = 0\n",
    "    for i in range(len(weights)-1):\n",
    "        activation += weights[i] * inputs[i]\n",
    "    activation += bias\n",
    "    return activation\n",
    " \n",
    "# Activation function - Sigmoid function\n",
    "def activation_function(activation):\n",
    "    return 1.0 / (1.0 + exp(-activation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the forward pass, we need to calculate the output of each layer for each neuron. All of the outputs from one layer become inputs to the neurons of the next layer.\n",
    "\n",
    "Propose an implementation of the $forward\\_propagate$ function which takes as argument a network and an input row. It has to store for each neuron a new $output$ key in the $network$ dictionnries and returns it as the next input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2: Implement forward propagation with the proposed schema.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagate input row to a network output - store each intermediate output of each neuron\n",
    "def forward_propagate(network, row):\n",
    "    '''\n",
    "    ####################################### ADD YOUR CODE HERE #######################################\n",
    "    INPUTS: network: model as definied in initialize_dense_network function\n",
    "            row: input for the model to forward propagate\n",
    "    Stores for each neuron a new output key in network\n",
    "    RETURN: calculated outputs of the last layer\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test forward propagation\n",
    "network = initialize_dense_network(2, 1, [2], 2)\n",
    "\n",
    "print('Network before forward propagation :')\n",
    "pprint(network)\n",
    "\n",
    "row = [1, 0]\n",
    "output = forward_propagate(network, row)\n",
    "print('\\nNetwork after forward propagation :')\n",
    "pprint(network)\n",
    "\n",
    "print('\\nNetwork output after forward propagation :')\n",
    "pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1.c Backpropagation of the error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By storing intermediate outputs of layers, we can now easily back propagate the result of the least square loss differentiation with respect to network parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the backpropagation we need the derivative of the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the derivative of an neuron output - Sigmoid function case\n",
    "def activation_function_derivative(output):\n",
    "    return output * (1.0 - output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3: Explain the definition of the $activation\\_function\\_derivative$ function on the derivative of the sigmoid function.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to calculate the error for each output neuron. The error for the output layer can be calculated as follows:\n",
    "\n",
    "$error = (expected - output) * activation\\_function\\_derivative(output)$\n",
    "\n",
    "where expected is the expected output value for the neuron (the class value itself), output is the output value for the neuron and $activation\\_function\\_derivative()$ calculates the slope of the neuronâ€™s output value.\n",
    "\n",
    "The error signal for a neuron in the hidden layer is calculated as the weighted error of each neuron in the output layer.\n",
    "\n",
    "The back-propagated error signal is accumulated and then used to determine the error for the $kth$ neuron in the hidden layer, as follows:\n",
    "\n",
    "$error_k = (\\sum_{j} weight_{kj} * error_j) * activation\\_function\\_derivative(output_k)$\n",
    "\n",
    "where:\n",
    "- $error_j$ is the error signal from the $jth$ neuron in the output layer\n",
    "- $weight_{kj}$ is the weight that connects the $kth$ neuron in the input layer to the $jth$ neuron in the output layer\n",
    "- $output_k$ is the output for the $kth$ neuron in the input layer.\n",
    "\n",
    "We are going to implement this procedure in a function named $backward\\_propagate\\_error()$.\n",
    "\n",
    "We want to store the error signal calculated for each neuron in each layer (without matrix computation -- similar to the $forward\\_propagate()$ function) with the name $delta$.\n",
    "\n",
    "TIP : Starting from the end of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4: Implement back propagation with the proposed schema**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network, expected):\n",
    "    '''\n",
    "    ####################################### ADD YOUR CODE HERE #######################################\n",
    "    INPUTS: network: model as definied in initialize_dense_network function\n",
    "            expected: ground truth vector\n",
    "    Stores for each neuron a new delta key in network representing the propagated error\n",
    "    RETURN: None\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test backpropagation of error\n",
    "print('Network before backpropagation :')\n",
    "pprint(network)\n",
    "\n",
    "expected = [0, 1]\n",
    "backward_propagate_error(network, expected)\n",
    "print('\\nNetwork after backpropagation :')\n",
    "pprint(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1.d Update the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once errors are calculated for each neuron in the network via the back propagation method above, they can be used to update weights.\n",
    "\n",
    "We will first update network weights with a simple schema as follows:\n",
    "\n",
    "$weight = weight + learning\\_rate * error * input$\n",
    "\n",
    "where $weight$ is a given weight, $learning\\_rate$ is a parameter that you must specify, $error$ is the error calculated by the backpropagation procedure for the neuron and input is the input value that produced this error.\n",
    "\n",
    "Do no forget to update the bias weight (Be careful the differentation term toward this weight is different).\n",
    "\n",
    "Below is a function named $update\\_weights()$ that updates the weights for a network given an input row of data, a learning rate and assume that a forward and backward propagation have already been performed.\n",
    "\n",
    "Remember that the input for the output layer is a collection of outputs from the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update network weights with error\n",
    "def update_weights(network, row, l_rate):\n",
    "    for i in range(len(network)):\n",
    "        # Discard last dimension because containing groundtruth label.\n",
    "        inputs = row[:-1]\n",
    "        if i != 0:\n",
    "            # After the first layer inputs become the output of the previous layer\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "            neuron['weights'][-1] += l_rate * neuron['delta']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the functions we need to train our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(network, train, l_rate, n_epoch, size_outputs):\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        for row in train:\n",
    "            label = int(row[-1])\n",
    "            outputs = forward_propagate(network, row)\n",
    "            \n",
    "            # Handle case where forward_propagate returns None\n",
    "            if outputs is None:\n",
    "                print(f\"Warning: forward_propagate returned None for input row: {row}\")\n",
    "                continue  # Skip this training example and continue with the next one\n",
    "            \n",
    "            # Create one_hot encoded vector for expected output\n",
    "            expected = [0 for i in range(size_outputs)]\n",
    "            expected[label] = 1\n",
    "            \n",
    "            try:\n",
    "                sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
    "                backward_propagate_error(network, expected)\n",
    "                update_weights(network, row, l_rate)\n",
    "            except Exception as e:\n",
    "                print(f\"Error during training on row {row}: {str(e)}\")\n",
    "                continue\n",
    "            \n",
    "        if epoch % 50 == 0:\n",
    "            print(' '*4+'Train Epoch = {}, Loss = {:.3f}'.format(epoch, sum_error))\n",
    "            \n",
    "def train_test_split(dataset, test_size=2):\n",
    "    if len(dataset) <= test_size:\n",
    "        raise ValueError(\"Dataset size must be greater than test_size\")\n",
    "    train_set = list(dataset[:-test_size])\n",
    "    test_set = list(dataset[-test_size:])\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset in train and test sets\n",
    "train_set, test_set = train_test_split(sdataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train network with the train set\n",
    "size_inputs = len(train_set[0]) - 1\n",
    "n_hidden_layers = 1\n",
    "sizes_hidden_layers =  [3]\n",
    "size_outputs = 2\n",
    "\n",
    "l_rate = 0.5\n",
    "n_epoch = 300\n",
    "\n",
    "network = initialize_dense_network(size_inputs, n_hidden_layers, sizes_hidden_layers, size_outputs)\n",
    "train_network(network, train_set, l_rate, n_epoch, size_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make a prediction with our network on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction in the test set\n",
    "def predict(network, row):\n",
    "    outputs = forward_propagate(network, row)\n",
    "    if outputs is None:\n",
    "        print(f\"Warning: forward_propagate returned None for input row: {row}\")\n",
    "        return None \n",
    "    return outputs.index(max(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in test_set:\n",
    "    prediction = predict(network, row)\n",
    "    if prediction is None:\n",
    "        print(f\"Warning: forward_propagate returned None for input row: {row}\")\n",
    "    else:  \n",
    "        print('Expected=%d, Got=%d' % (row[-1], prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Wheat seeds dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to test our optimization algorithm on a more complex dataset.\n",
    "\n",
    "As a new dataset, we will use the seeds dataset that consists of measurements of different wheat species.\n",
    "\n",
    "There are $201$ records and $7$ numerical input variables. It is a classification problem with $3$ output classes. The scale for each numeric input value vary. As in many machine learning methods, data normalization could be very helpful.\n",
    "\n",
    "Using the Zero Rule algorithm that predicts the most common class value, the baseline accuracy for the problem is $28.095\\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1. Our basic python implementaion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate the algorithm using k-fold cross-validation with $5$ folds. This means that $201/5=40.2$ or $40$ records will be in each fold. We will use the helper functions $evaluate\\_algorithm()$ to evaluate the algorithm with cross-validation and $accuracy\\_metric()$ to calculate the accuracy of predictions.\n",
    "\n",
    "A new function named $back\\_propagation()$was developed to manage the application of the Backpropagation algorithm, first initializing a network, training it on the training dataset and then using the trained network to make predictions on a test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    " \n",
    "# Find the min and max values for each column\n",
    "def dataset_minmax(dataset):\n",
    "    minmax = list()\n",
    "    stats = [[min(column), max(column)] for column in zip(*dataset)]\n",
    "    return stats\n",
    " \n",
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize_dataset(dataset, minmax):\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)-1):\n",
    "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for i in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    " \n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual))\n",
    " \n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for it_fold, fold in enumerate(folds):\n",
    "        print('\\nCross Validation - Fold {}/{}'.format(it_fold+1, len(folds)))\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        val_set = fold\n",
    "        predicted = algorithm(train_set, val_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores\n",
    "\n",
    "# Backpropagation Algorithm With Stochastic Gradient Descent\n",
    "def back_propagation(train, val, l_rate, n_epoch, n_hidden_layers, sizes_hidden_layers):\n",
    "    size_inputs = len(train[0]) - 1\n",
    "    size_outputs = len(set([row[-1] for row in train]))\n",
    "    network = initialize_dense_network(size_inputs, n_hidden_layers, sizes_hidden_layers, size_outputs)\n",
    "    train_network(network, train, l_rate, n_epoch, size_outputs)\n",
    "    predictions = list()\n",
    "    for row in val:\n",
    "        prediction = predict(network, row)\n",
    "        predictions.append(prediction)\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "import pandas as pd\n",
    "data = pd.read_csv('seeds_dataset.txt', sep=\"\\t\", header=None)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform class labels to [0, n_labels-1]\n",
    "wdataset = data.to_numpy()\n",
    "wdataset[:,-1] = wdataset[:,-1]-1\n",
    "wdataset = wdataset.tolist()\n",
    "\n",
    "# Normalize input variables\n",
    "minmax = dataset_minmax(wdataset)\n",
    "normalize_dataset(wdataset, minmax)\n",
    "\n",
    "# Evaluate algorithm\n",
    "n_folds = 5\n",
    "l_rate = 0.5\n",
    "n_epoch = 500\n",
    "n_hidden_layers = 2\n",
    "sizes_hidden_layers =  [4, 4]\n",
    "\n",
    "scores = evaluate_algorithm(wdataset, back_propagation, n_folds, l_rate, n_epoch, n_hidden_layers, sizes_hidden_layers)\n",
    "print('\\nScores: {}'.format(['{:.2%}'.format(s) for s in scores]))\n",
    "print('Mean Accuracy: {:.2%}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2. Implementation with pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the section, we propose to solve the same classification problem with a pytorch implementation.\n",
    "\n",
    "If you work locally, install PyTorch using the instructions that are available here : https://pytorch.org/get-started/locally/\n",
    "\n",
    "Do not hesitate to check the available documentation and tutorials!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# One-hot encoding of the data in order to easily use pytorch data loaders\n",
    "one_hot = pd.get_dummies(data.iloc[:,-1], prefix='label')\n",
    "data_one_hot = pd.concat([data.drop(labels=[7], axis=1), one_hot], axis=1)\n",
    "data_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize class labels to [0, n_labels-1]\n",
    "wdataset_one_hot = data_one_hot.to_numpy()\n",
    "wdataset_one_hot = wdataset_one_hot.tolist()\n",
    "\n",
    "# normalize input variables\n",
    "minmax = dataset_minmax(wdataset_one_hot)\n",
    "normalize_dataset(wdataset_one_hot, minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pytorch, you can define your network architecture as class of the $torch.nn.Module$.\n",
    "\n",
    "You just have to define the forward function, and the backward function where gradients are computed automatically using autograd. You can use any of the Tensor operations in the forward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a neural network architecture\n",
    "class dense_network(nn.Module):\n",
    "    def __init__(self, size_inputs, n_hidden_layers, sizes_hidden_layers, size_outputs):\n",
    "        super(dense_network, self).__init__()\n",
    "        # Input layer\n",
    "        sizes_hidden_layers = sizes_hidden_layers+[sizes_hidden_layers[-1]]\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.input_layer = nn.Linear(size_inputs, sizes_hidden_layers[0])\n",
    "        \n",
    "        # Hidden layers as a list module\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for n_hidden_layer in range(n_hidden_layers):\n",
    "            self.hidden_layers.append( nn.Linear(sizes_hidden_layers[n_hidden_layer], sizes_hidden_layers[n_hidden_layer+1]))\n",
    "            \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(sizes_hidden_layers[-1], size_outputs)\n",
    "                                      \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.input_layer(x))\n",
    "        for n_hidden_layer in range(self.n_hidden_layers):\n",
    "            x = torch.sigmoid(self.hidden_layers[n_hidden_layer](x))\n",
    "        x = torch.sigmoid(self.output_layer(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize network\n",
    "size_inputs = 7\n",
    "n_hidden_layers = 2\n",
    "sizes_hidden_layers =  [4, 4]\n",
    "size_outputs = 3\n",
    "\n",
    "network = dense_network(size_inputs, n_hidden_layers, sizes_hidden_layers, size_outputs)\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the weights according to the parameters of the architecture you initialized\n",
    "params = list(network.parameters())\n",
    "print('Parameters:\\n', params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define a cross validation function which train and test the network for each fold of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train function\n",
    "def pytorch_train(network, loader, optimizer, criterion, n_epoch):\n",
    "    network.train()\n",
    "    for epoch in range(n_epoch):\n",
    "        train_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(loader):\n",
    "            optimizer.zero_grad()\n",
    "            net_out = network(data)\n",
    "            loss = criterion(net_out, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        if epoch % 100 == 0:\n",
    "            print(' '*4+'Train Epoch = {}, error = {:.3f}'.format(epoch, train_loss))\n",
    "\n",
    "# Evaluation function\n",
    "def pytorch_validation(network, loader, optimizer, criterion):\n",
    "    eval_loss = 0\n",
    "    correct = 0\n",
    "    network.eval()\n",
    "    for data, target in loader:\n",
    "        net_out = network(data)\n",
    "        eval_loss += criterion(net_out, target).item()\n",
    "        idx = net_out.data.max(1)[1] \n",
    "        pred = torch.Tensor([0, 0, 0])\n",
    "        pred[idx] = 1\n",
    "        if (pred.numpy() == target.numpy()).all():\n",
    "            correct += 1\n",
    "    correct /= len(loader.dataset)\n",
    "    eval_loss /= len(loader.dataset)\n",
    "    return correct\n",
    "\n",
    "def pytorch_cross_validation(dataset, n_epoch, l_rate, n_folds, size_outputs):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for it_fold, fold in enumerate(folds):\n",
    "        # Initialize network\n",
    "        size_inputs = 7\n",
    "        n_hidden_layers = 2\n",
    "        sizes_hidden_layers =  [4, 4]\n",
    "        size_outputs = 3\n",
    "        network = dense_network(size_inputs, n_hidden_layers, sizes_hidden_layers, size_outputs)\n",
    "        \n",
    "        # Initialize stochastic gradient descent optimizer\n",
    "        optimizer = optim.SGD(network.parameters(), lr=l_rate)\n",
    "        # Initialize loss function\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        print('\\nCross Validation - Fold {}/{}'.format(it_fold+1, len(folds)))\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = np.array(sum(train_set, []))\n",
    "        val_set = np.array(fold)\n",
    "        \n",
    "        # Build pytorch data train and validation loaders\n",
    "        train_set = torch.tensor(np.array(train_set)).float()\n",
    "        train_set = torch.utils.data.TensorDataset(train_set[:,:-3], train_set[:,-3:])\n",
    "        train_loader = torch.utils.data.DataLoader(train_set)\n",
    "        val_set = torch.tensor(np.array(val_set)).float()\n",
    "        val_set = torch.utils.data.TensorDataset(val_set[:,:-3], val_set[:,-3:])\n",
    "        val_loader = torch.utils.data.DataLoader(val_set)\n",
    "        \n",
    "        # train and evaluate network on fold\n",
    "        pytorch_train(network, val_loader, optimizer, criterion, n_epoch)\n",
    "        val_score = pytorch_validation(network, val_loader, optimizer, criterion)\n",
    "        scores.append(val_score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "n_folds = 5\n",
    "n_epoch = 600\n",
    "l_rate = 0.5\n",
    "scores = pytorch_cross_validation(wdataset_one_hot, n_epoch, l_rate, n_folds, size_outputs)\n",
    "print('\\nScores: {}'.format(['{:.2%}'.format(s) for s in scores]))\n",
    "print('Mean Accuracy: {:.2%}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III CIFAR-10 Data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will tackle a more difficult problem: a classification task on the $CIFAR-10$ data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cifar_10_examples.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gain some time, the next cell proposes a pytorch dataloader from the modified file located at https://github.com/pytorch/vision/blob/master/torchvision/datasets/cifar.py in order to obtain a training loader that will only use the first 100 samples of the CIFAR-10 training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "import sys\n",
    "import hashlib\n",
    "import errno\n",
    "from torch.utils.model_zoo import tqdm\n",
    "if sys.version_info[0] == 2:\n",
    "    import cPickle as pickle\n",
    "else:\n",
    "    import pickle\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "\n",
    "def download_url(url, root, filename=None, md5=None):\n",
    "    \"\"\"Download a file from a url and place it in root.\n",
    "    Args:\n",
    "        url (str): URL to download file from\n",
    "        root (str): Directory to place downloaded file in\n",
    "        filename (str, optional): Name to save the file under. If None, use the basename of the URL\n",
    "        md5 (str, optional): MD5 checksum of the download. If None, do not check\n",
    "    \"\"\"\n",
    "    from six.moves import urllib\n",
    "    root = os.path.expanduser(root)\n",
    "    if not filename:\n",
    "        filename = os.path.basename(url)\n",
    "    fpath = os.path.join(root, filename)\n",
    "    makedir_exist_ok(root)\n",
    "    # downloads file\n",
    "    if os.path.isfile(fpath) and check_integrity(fpath, md5):\n",
    "        print('Using downloaded and verified file: ' + fpath)\n",
    "    else:\n",
    "        try:\n",
    "            print('Downloading ' + url + ' to ' + fpath)\n",
    "            urllib.request.urlretrieve(\n",
    "                url, fpath,\n",
    "                reporthook=gen_bar_updater()\n",
    "            )\n",
    "        except OSError:\n",
    "            if url[:5] == 'https':\n",
    "                url = url.replace('https:', 'http:')\n",
    "                print('Failed download. Trying https -> http instead.'\n",
    "                      ' Downloading ' + url + ' to ' + fpath)\n",
    "                urllib.request.urlretrieve(\n",
    "                    url, fpath,\n",
    "                    reporthook=gen_bar_updater()\n",
    "                )\n",
    "                \n",
    "def makedir_exist_ok(dirpath):\n",
    "    \"\"\"\n",
    "    Python2 support for os.makedirs(.., exist_ok=True)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.makedirs(dirpath)\n",
    "    except OSError as e:\n",
    "        if e.errno == errno.EEXIST:\n",
    "            pass\n",
    "        else:\n",
    "            raise\n",
    "            \n",
    "def gen_bar_updater():\n",
    "    pbar = tqdm(total=None)\n",
    "    def bar_update(count, block_size, total_size):\n",
    "        if pbar.total is None and total_size:\n",
    "            pbar.total = total_size\n",
    "        progress_bytes = count * block_size\n",
    "        pbar.update(progress_bytes - pbar.n)\n",
    "    return bar_update\n",
    "\n",
    "def check_integrity(fpath, md5=None):\n",
    "    if md5 is None:\n",
    "        return True\n",
    "    if not os.path.isfile(fpath):\n",
    "        return False\n",
    "    md5o = hashlib.md5()\n",
    "    with open(fpath, 'rb') as f:\n",
    "        # read in 1MB chunks\n",
    "        for chunk in iter(lambda: f.read(1024 * 1024), b''):\n",
    "            md5o.update(chunk)\n",
    "    md5c = md5o.hexdigest()\n",
    "    if md5c != md5:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "class CIFAR10(data.Dataset):\n",
    "    \"\"\"`CIFAR10 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
    "    Args:\n",
    "        root (string): Root directory of dataset where directory\n",
    "            ``cifar-10-batches-py`` exists or will be saved to if download is set to True.\n",
    "        transform (callable, optional): A function/transform that takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        download (bool, optional): If true, downloads the dataset from the internet and\n",
    "            puts it in root directory. If dataset is already downloaded, it is not\n",
    "            downloaded again.\n",
    "    \"\"\"\n",
    "    base_folder = 'cifar-10-batches-py'\n",
    "    url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "    filename = \"cifar-10-python.tar.gz\"\n",
    "    tgz_md5 = 'c58f30108f718f92721af3b95e74349a'\n",
    "    train_list = [\n",
    "        ['data_batch_1', 'c99cafc152244af753f735de768cd75f'],\n",
    "        ['data_batch_2', 'd4bba439e000b95fd0a9bffe97cbabec'],\n",
    "        ['data_batch_3', '54ebc095f3ab1f0389bbae665268c751'],\n",
    "        ['data_batch_4', '634d18415352ddfa80567beed471001a'],\n",
    "        ['data_batch_5', '482c414d41f54cd18b22e5b47cb7c3cb'],\n",
    "    ]\n",
    "    test_list = [\n",
    "        ['test_batch', '40351d587109b95175f43aff81a1287e'],\n",
    "    ]\n",
    "    meta = {\n",
    "        'filename': 'batches.meta',\n",
    "        'key': 'label_names',\n",
    "        'md5': '5ff9c542aee3614f3951f8cda6e48888',\n",
    "    }\n",
    "    def __init__(self, root, download=False, dset='train', crop=True, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.dset = dset\n",
    "        self.crop = crop\n",
    "        if download:\n",
    "            self.download()\n",
    "        if self.dset == 'train':\n",
    "            downloaded_list = self.train_list\n",
    "        elif self.dset == 'test':\n",
    "            downloaded_list = self.test_list\n",
    "        self.data = []\n",
    "        self.targets = []\n",
    "        # now load the picked numpy arrays\n",
    "        for file_name, checksum in downloaded_list:\n",
    "            file_path = os.path.join(self.root, self.base_folder, file_name)\n",
    "            with open(file_path, 'rb') as f:\n",
    "                if sys.version_info[0] == 2:\n",
    "                    entry = pickle.load(f)\n",
    "                else:\n",
    "                    entry = pickle.load(f, encoding='latin1')\n",
    "                self.data.append(entry['data'])\n",
    "                if 'labels' in entry:\n",
    "                    self.targets.extend(entry['labels'])\n",
    "                else:\n",
    "                    self.targets.extend(entry['fine_labels'])\n",
    "        self.data = np.vstack(self.data).reshape(-1, 3, 32, 32)\n",
    "        self.data = self.data.transpose((0, 2, 3, 1))  # convert to HWC\n",
    "        if self.crop == True and self.dset == 'train':\n",
    "            self.data = self.data[0:100]\n",
    "            self.targets = self.targets[0:100]\n",
    "        elif self.crop == False and self.dset == 'train':\n",
    "            self.data = self.data[100:]\n",
    "            self.targets = self.targets[100:]\n",
    "        self._load_meta()\n",
    "        \n",
    "    def _load_meta(self):\n",
    "        path = os.path.join(self.root, self.base_folder, self.meta['filename'])\n",
    "        if not check_integrity(path, self.meta['md5']):\n",
    "            raise RuntimeError('Dataset metadata file not found or corrupted.' +\n",
    "                               ' You can use download=True to download it')\n",
    "        with open(path, 'rb') as infile:\n",
    "            if sys.version_info[0] == 2:\n",
    "                data = pickle.load(infile)\n",
    "            else:\n",
    "                data = pickle.load(infile, encoding='latin1')\n",
    "            self.classes = data[self.meta['key']]\n",
    "        self.class_to_idx = {_class: i for i, _class in enumerate(self.classes)}\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def download(self):\n",
    "        import tarfile\n",
    "        download_url(self.url, self.root, self.filename, self.tgz_md5)\n",
    "        # extract file\n",
    "        with tarfile.open(os.path.join(self.root, self.filename), \"r:gz\") as tar:\n",
    "            tar.extractall(path=self.root)\n",
    "            \n",
    "    def extra_repr(self):\n",
    "        return \"Split: {}\".format(\"Train\" if self.train is True else \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = os.getcwd()\n",
    "import torch\n",
    "\n",
    "# Transformations applied to img\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Get train, val datasets and dataloaders\n",
    "x_train = CIFAR10(path_to_data, download=True, dset='train', crop=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(x_train, batch_size=32, shuffle=True)\n",
    "x_test = CIFAR10(path_to_data, download=True, dset='test', crop=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(x_test, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5: Propose a simple CNN architecture and train it with the proposed loader. Try to achieve the best test accuracy !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    '''\n",
    "    ####################################### ADD YOUR CODE HERE #######################################\n",
    "    Pytorch network definition: see documentation, tutorials or examples for help.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "####################################### ADD YOUR CODE HERE #######################################\n",
    "Define your own taining, testing and scoring functions.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV Try to fail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6: Now that we have successfully trained both our network with our costum functions for back propagation and with the functions provided by pytorch, try to change the hyperparameters of the network in order to make its optimization to fail.**\n",
    "\n",
    "**You can use either your python implementation or the pytorch one.**\n",
    "\n",
    "**You can mainly play on the parameters of:**\n",
    "- **network architecture**\n",
    "- **optimization & training**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "####################################### ADD YOUR CODE HERE #######################################\n",
    "Use the code, functions proposed in this notebook and play on their parameters.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
